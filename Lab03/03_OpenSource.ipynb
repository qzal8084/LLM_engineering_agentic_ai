{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8557b86",
   "metadata": {},
   "source": [
    "# ðŸ§­ 1. What Is a Frontier Model?\n",
    "\n",
    "A frontier model is the most advanced, cutting-edge type of AI system available at a given time.\n",
    "These are the models that push the boundaries (â€œfrontierâ€) of what AI can do.\n",
    "\n",
    "âœ”ï¸ Characteristics of Frontier Models\n",
    "\n",
    "Frontier models usually:\n",
    "\n",
    "- use massive scale (billionsâ€“trillions of parameters)\n",
    "- require huge compute to train\n",
    "- achieve state-of-the-art performance on reasoning, math, coding, and knowledge tasks\n",
    "- are often closed-source (not publicly released)\n",
    "- include safety features, RLHF, and aligned behavior\n",
    "- come from big AI labs with huge resources (OpenAI, Google, Anthropic)\n",
    "\n",
    "âœ”ï¸ Examples of Frontier Models\n",
    "\n",
    "- GPT-4, GPT-4o, GPT-5 (OpenAI)\n",
    "- Claude 3/3.5 (Anthropic)\n",
    "- Gemini 1.5/2.0 (Google)\n",
    "\n",
    "Think of frontier models as:\n",
    "\n",
    "â€œThe highest-performance, cutting-edge AI models available.â€\n",
    "\n",
    "# ðŸŸ¦ 2. What Is an Open-Source LLM?\n",
    "\n",
    "An open-source model is one where:\n",
    "\n",
    "- the model weights are publicly released\n",
    "- the architecture is documented\n",
    "- developers can download, run, modify, and fine-tune the model locally\n",
    "- often free to use (with some license rules)\n",
    "\n",
    "âœ”ï¸ Characteristics of Open-Source Models\n",
    "\n",
    "Open-source LLMs:\n",
    "\n",
    "- are often smaller (but rapidly improving)\n",
    "- can be run locally (e.g., on your laptop or server)\n",
    "- allow full customization\n",
    "- have community-driven development\n",
    "- provide transparency for research, auditing, and safety\n",
    "\n",
    "âœ”ï¸ Examples of Open-Source Models\n",
    "\n",
    "- Llama 3 / Llama 3.1 (Meta)\n",
    "- Mistral 7B /\n",
    "- Gemma (Google)\n",
    "- Qwen\n",
    "- Phi-3\n",
    "\n",
    "Think of open-source models as:\n",
    "\n",
    "â€œAI models that anyone can download, modify, and inspect.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def617b0",
   "metadata": {},
   "source": [
    "# What is Hugging Face?\n",
    "\n",
    "Hugging Face is the main platform for open-source AI.\n",
    "\n",
    "âœ”ï¸ Think of Hugging Face as:\n",
    "\n",
    "- A GitHub for AI models\n",
    "- A community of AI developers\n",
    "- A giant library of machine learning resources\n",
    "\n",
    "âœ”ï¸ What you can find on Hugging Face:\n",
    "\n",
    "- Thousands of open-source LLMs\n",
    "- Datasets for training\n",
    "- Pretrained models for vision, audio, NLP\n",
    "- Tools for fine-tuning and deployment\n",
    "- Model evaluation leaderboards\n",
    "\n",
    "âœ”ï¸ Why is it important?\n",
    "\n",
    "Hugging Face makes it easy to:\n",
    "\n",
    "- Download pre-trained models\n",
    "- Try models in your browser\n",
    "- Use models in code\n",
    "- Share your own models with the community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f68a8e3",
   "metadata": {},
   "source": [
    "# What is Ollama\n",
    "\n",
    "Ollama is a tool that lets you easily run open-source LLMs on your own device.\n",
    "\n",
    "âœ”ï¸ Why is Ollama useful?\n",
    "\n",
    "- Runs models locally (no internet required)\n",
    "- Easy installation of models\n",
    "- Good for privacy (your data stays on your device)\n",
    "- Works with many open-source models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956df924",
   "metadata": {},
   "source": [
    "### Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`. \n",
    "\n",
    "On your terminal:\n",
    "```\n",
    ">>> ollama\n",
    ">>> ollama list\n",
    ">>> ollama run llama3 \n",
    "```\n",
    "\n",
    "Install Python package:\n",
    "```\n",
    "pip install ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16029fa2",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e7eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"deepseek-r1:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d05b32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85be5651",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d2eb3",
   "metadata": {},
   "source": [
    "### Client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2028e2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The answer to \\(2 + 2\\) is **4**.\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The answer to \\(2 + 2\\) is **4**.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model=MODEL, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'what is 2+2?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea94021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
